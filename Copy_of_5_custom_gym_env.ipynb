{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of 5.custom_gym_env.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KevivJaknap/Story/blob/master/Copy_of_5_custom_gym_env.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoxOjIlOImwx"
      },
      "source": [
        "# Stable Baselines Tutorial - Creating a custom Gym environment\n",
        "\n",
        "Github repo: https://github.com/araffin/rl-tutorial-jnrr19\n",
        "\n",
        "Stable-Baselines: https://github.com/hill-a/stable-baselines\n",
        "\n",
        "Documentation: https://stable-baselines.readthedocs.io/en/master/\n",
        "\n",
        "RL Baselines zoo: https://github.com/araffin/rl-baselines-zoo\n",
        "\n",
        "\n",
        "## Introduction\n",
        "\n",
        "In this notebook, you will learn how to use your own environment following the OpenAI Gym interface.\n",
        "Once it is done, you can easily use any compatible (depending on the action space) RL algorithm from Stable Baselines on that environment.\n",
        "\n",
        "## Install Dependencies and Stable Baselines Using Pip\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sp8rSS4DIhEV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7dfa387-c5e8-4d06-ec60-7236548cf150"
      },
      "source": [
        "# Stable Baselines only supports tensorflow 1.x for now\n",
        "%tensorflow_version 1.x\n",
        "!pip install stable-baselines[mpi]==2.10.0"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: Tensorflow 1 is deprecated, and support will be removed on August 1, 2022.\n",
            "After that, `%tensorflow_version 1.x` will throw an error.\n",
            "\n",
            "Your notebook should be updated to use Tensorflow 2.\n",
            "See the guide at https://www.tensorflow.org/guide/migrate#migrate-from-tensorflow-1x-to-tensorflow-2.\n",
            "\n",
            "TensorFlow 1.x selected.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting stable-baselines[mpi]==2.10.0\n",
            "  Downloading stable_baselines-2.10.0-py3-none-any.whl (248 kB)\n",
            "\u001b[K     |████████████████████████████████| 248 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (3.2.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (4.6.0.66)\n",
            "Requirement already satisfied: cloudpickle>=0.5.5 in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.1.0)\n",
            "Requirement already satisfied: gym[atari,classic_control]>=0.11 in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (0.17.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.3.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.7.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.21.6)\n",
            "Requirement already satisfied: mpi4py in /tensorflow-1.15.2/python3.7 (from stable-baselines[mpi]==2.10.0) (3.0.3)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (1.5.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (7.1.2)\n",
            "Requirement already satisfied: atari-py~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (0.2.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from atari-py~=0.2.0->gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (1.15.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (0.16.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (0.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->stable-baselines[mpi]==2.10.0) (4.1.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->stable-baselines[mpi]==2.10.0) (2022.1)\n",
            "Installing collected packages: stable-baselines\n",
            "  Attempting uninstall: stable-baselines\n",
            "    Found existing installation: stable-baselines 2.2.1\n",
            "    Uninstalling stable-baselines-2.2.1:\n",
            "      Successfully uninstalled stable-baselines-2.2.1\n",
            "Successfully installed stable-baselines-2.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzevZcgmJmhi"
      },
      "source": [
        "## First steps with the gym interface\n",
        "\n",
        "As you have noticed in the previous notebooks, an environment that follows the gym interface is quite simple to use.\n",
        "It provides to this user mainly three methods:\n",
        "- `reset()` called at the beginning of an episode, it returns an observation\n",
        "- `step(action)` called to take an action with the environment, it returns the next observation, the immediate reward, whether the episode is over and additional information\n",
        "- (Optional) `render(method='human')` which allow to visualize the agent in action. Note that graphical interface does not work on google colab, so we cannot use it directly (we have to rely on `method='rbg_array'` to retrieve an image of the scene\n",
        "\n",
        "Under the hood, it also contains two useful properties:\n",
        "- `observation_space` which one of the gym spaces (`Discrete`, `Box`, ...) and describe the type and shape of the observation\n",
        "- `action_space` which is also a gym space object that describes the action space, so the type of action that can be taken\n",
        "\n",
        "The best way to learn about gym spaces is to look at the [source code](https://github.com/openai/gym/tree/master/gym/spaces), but you need to know at least the main ones:\n",
        "- `gym.spaces.Box`: A (possibly unbounded) box in $R^n$. Specifically, a Box represents the Cartesian product of n closed intervals. Each interval has the form of one of [a, b], (-oo, b], [a, oo), or (-oo, oo). Example: A 1D-Vector or an image observation can be described with the Box space.\n",
        "```python\n",
        "# Example for using image as input:\n",
        "observation_space = spaces.Box(low=0, high=255, shape=(HEIGHT, WIDTH, N_CHANNELS), dtype=np.uint8)\n",
        "```                                       \n",
        "\n",
        "- `gym.spaces.Discrete`: A discrete space in $\\{ 0, 1, \\dots, n-1 \\}$\n",
        "  Example: if you have two actions (\"left\" and \"right\") you can represent your action space using `Discrete(2)`, the first action will be 0 and the second 1.\n",
        "\n",
        "\n",
        "\n",
        "[Documentation on custom env](https://stable-baselines.readthedocs.io/en/master/guide/custom_env.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I98IKKyNJl6K",
        "outputId": "1f66473b-5b4c-4773-c481-befe7f4df98a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import gym\n",
        "\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "\n",
        "# Box(4,) means that it is a Vector with 4 components\n",
        "print(\"Observation space:\", env.observation_space)\n",
        "print(\"Shape:\", env.observation_space.shape)\n",
        "# Discrete(2) means that there is two discrete actions\n",
        "print(\"Action space:\", env.action_space)\n",
        "\n",
        "# The reset method is called at the beginning of an episode\n",
        "obs = env.reset()\n",
        "# Sample a random action\n",
        "action = env.action_space.sample()\n",
        "print(\"Sampled action:\", action)\n",
        "obs, reward, done, info = env.step(action)\n",
        "# Note the obs is a numpy array\n",
        "# info is an empty dict for now but can contain any debugging info\n",
        "# reward is a scalar\n",
        "print(obs.shape, reward, done, info)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Observation space: Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\n",
            "Shape: (4,)\n",
            "Action space: Discrete(2)\n",
            "Sampled action: 1\n",
            "(4,) 1.0 False {}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqxatIwPOXe_"
      },
      "source": [
        "##  Gym env skeleton\n",
        "\n",
        "In practice this is how a gym environment looks like.\n",
        "Here, we have implemented a simple grid world were the agent must learn to go always left."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYzDXA9vJfz1"
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "from gym import spaces\n",
        "\n",
        "\n",
        "class GoLeftEnv(gym.Env):\n",
        "  \"\"\"\n",
        "  Custom Environment that follows gym interface.\n",
        "  This is a simple env where the agent must learn to go always left. \n",
        "  \"\"\"\n",
        "  # Because of google colab, we cannot implement the GUI ('human' render mode)\n",
        "  metadata = {'render.modes': ['console']}\n",
        "  # Define constants for clearer code\n",
        "  LEFT = 0\n",
        "  RIGHT = 1\n",
        "\n",
        "  def __init__(self, grid_size=10):\n",
        "    super(GoLeftEnv, self).__init__()\n",
        "\n",
        "    # Size of the 1D-grid\n",
        "    self.grid_size = grid_size\n",
        "    # Initialize the agent at the right of the grid\n",
        "    self.agent_pos = grid_size - 1\n",
        "\n",
        "    # Define action and observation space\n",
        "    # They must be gym.spaces objects\n",
        "    # Example when using discrete actions, we have two: left and right\n",
        "    n_actions = 2\n",
        "    self.action_space = spaces.Discrete(n_actions)\n",
        "    # The observation will be the coordinate of the agent\n",
        "    # this can be described both by Discrete and Box space\n",
        "    self.observation_space = spaces.Box(low=0, high=self.grid_size,\n",
        "                                        shape=(1,), dtype=np.float32)\n",
        "\n",
        "  def reset(self):\n",
        "    \"\"\"\n",
        "    Important: the observation must be a numpy array\n",
        "    :return: (np.array) \n",
        "    \"\"\"\n",
        "    # Initialize the agent at the right of the grid\n",
        "    self.agent_pos = self.grid_size - 1\n",
        "    # here we convert to float32 to make it more general (in case we want to use continuous actions)\n",
        "    return np.array([self.agent_pos]).astype(np.float32)\n",
        "\n",
        "  def step(self, action):\n",
        "    if action == self.LEFT:\n",
        "      self.agent_pos -= 1\n",
        "    elif action == self.RIGHT:\n",
        "      self.agent_pos += 1\n",
        "    else:\n",
        "      raise ValueError(\"Received invalid action={} which is not part of the action space\".format(action))\n",
        "\n",
        "    # Account for the boundaries of the grid\n",
        "    self.agent_pos = np.clip(self.agent_pos, 0, self.grid_size)\n",
        "\n",
        "    # Are we at the left of the grid?\n",
        "    done = bool(self.agent_pos == 0)\n",
        "\n",
        "    # Null reward everywhere except when reaching the goal (left of the grid)\n",
        "    reward = 1 if self.agent_pos == 0 else 0\n",
        "\n",
        "    # Optionally we can pass additional info, we are not using that for now\n",
        "    info = {}\n",
        "\n",
        "    return np.array([self.agent_pos]).astype(np.float32), reward, done, info\n",
        "\n",
        "  def render(self, mode='console'):\n",
        "    if mode != 'console':\n",
        "      raise NotImplementedError()\n",
        "    # agent is represented as a cross, rest as a dot\n",
        "    print(\".\" * self.agent_pos, end=\"\")\n",
        "    print(\"x\", end=\"\")\n",
        "    print(\".\" * (self.grid_size - self.agent_pos))\n",
        "\n",
        "  def close(self):\n",
        "    pass\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zy5mlho1-Ine"
      },
      "source": [
        "### Validate the environment\n",
        "\n",
        "Stable Baselines provides a [helper](https://stable-baselines.readthedocs.io/en/master/common/env_checker.html) to check that your environment follows the Gym interface. It also optionally checks that the environment is compatible with Stable-Baselines (and emits warning if necessary)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DOpP_B0-LXm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6ad21b8-4b91-4749-99fd-dff32055c065"
      },
      "source": [
        "from stable_baselines.common.env_checker import check_env"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CcUVatq-P0l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "outputId": "d44606e7-754d-4c6e-9f45-a98d6e73f097"
      },
      "source": [
        "env = GoLeftEnv()\n",
        "# If the environment don't follow the interface, an error will be thrown\n",
        "check_env(env, warn=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-b237822a9998>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGoLeftEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# If the environment don't follow the interface, an error will be thrown\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcheck_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'GoLeftEnv' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJ3khFtkSE0g"
      },
      "source": [
        "### Testing the environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i62yf2LvSAYY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6012c643-bcc5-48b3-83b9-dab9998d6106"
      },
      "source": [
        "env = GoLeftEnv(grid_size=10)\n",
        "\n",
        "obs = env.reset()\n",
        "env.render()\n",
        "\n",
        "print(env.observation_space)\n",
        "print(env.action_space)\n",
        "print(env.action_space.sample())\n",
        "\n",
        "GO_LEFT = 0\n",
        "# Hardcoded best agent: always go left!\n",
        "n_steps = 20\n",
        "for step in range(n_steps):\n",
        "  print(\"Step {}\".format(step + 1))\n",
        "  obs, reward, done, info = env.step(GO_LEFT)\n",
        "  print('obs=', obs, 'reward=', reward, 'done=', done)\n",
        "  env.render()\n",
        "  if done:\n",
        "    print(\"Goal reached!\", \"reward=\", reward)\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".........x.\n",
            "Box(0.0, 10.0, (1,), float32)\n",
            "Discrete(2)\n",
            "1\n",
            "Step 1\n",
            "obs= [8.] reward= 0 done= False\n",
            "........x..\n",
            "Step 2\n",
            "obs= [7.] reward= 0 done= False\n",
            ".......x...\n",
            "Step 3\n",
            "obs= [6.] reward= 0 done= False\n",
            "......x....\n",
            "Step 4\n",
            "obs= [5.] reward= 0 done= False\n",
            ".....x.....\n",
            "Step 5\n",
            "obs= [4.] reward= 0 done= False\n",
            "....x......\n",
            "Step 6\n",
            "obs= [3.] reward= 0 done= False\n",
            "...x.......\n",
            "Step 7\n",
            "obs= [2.] reward= 0 done= False\n",
            "..x........\n",
            "Step 8\n",
            "obs= [1.] reward= 0 done= False\n",
            ".x.........\n",
            "Step 9\n",
            "obs= [0.] reward= 1 done= True\n",
            "x..........\n",
            "Goal reached! reward= 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pv1e1qJETfHU"
      },
      "source": [
        "### Try it with Stable-Baselines\n",
        "\n",
        "Once your environment follow the gym interface, it is quite easy to plug in any algorithm from stable-baselines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQfLBE28SNDr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "a8bb4a0b-d1c5-483f-9400-3bb934e13e1e"
      },
      "source": [
        "from stable_baselines import DQN, PPO2, A2C, ACKTR\n",
        "from stable_baselines.common.cmd_util import make_vec_env\n",
        "\n",
        "# Instantiate the env\n",
        "env = GoLeftEnv(grid_size=10)\n",
        "# wrap it\n",
        "env = make_vec_env(lambda: env, n_envs=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRV4Q7FVUKB6"
      },
      "source": [
        "# Train the agent\n",
        "model = ACKTR('MlpPolicy', env, verbose=1).learn(5000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJbeiF0RUN-p"
      },
      "source": [
        "# Test the trained agent\n",
        "obs = env.reset()\n",
        "n_steps = 20\n",
        "for step in range(n_steps):\n",
        "  action, _ = model.predict(obs, deterministic=True)\n",
        "  print(\"Step {}\".format(step + 1))\n",
        "  print(\"Action: \", action)\n",
        "  obs, reward, done, info = env.step(action)\n",
        "  print('obs=', obs, 'reward=', reward, 'done=', done)\n",
        "  env.render(mode='console')\n",
        "  if done:\n",
        "    # Note that the VecEnv resets automatically\n",
        "    # when a done signal is encountered\n",
        "    print(\"Goal reached!\", \"reward=\", reward)\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vclHoDRy0xHd"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOggIa9sU--b"
      },
      "source": [
        "## It is your turn now, be creative!\n",
        "\n",
        "As an exercise, that's now your turn to build a custom gym environment.\n",
        "There is no constrain about what to do, be creative! (but not too creative, there is not enough time for that)\n",
        "\n",
        "If you don't have any idea, here is is a list of the environment you can implement:\n",
        "- Transform the discrete grid world to a continuous one, you will need to change a bit the logic and the action space\n",
        "- Create a 2D grid world and add walls\n",
        "- Create a tic-tac-toe game\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBDp4Pm-Uh4D"
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "from gym import spaces\n",
        "import random\n",
        "\n",
        "class TicTacToeEnv(gym.Env):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(TicTacToeEnv, self).__init__()\n",
        "    self.action_space = spaces.Discrete(9)\n",
        "    self.observation_space = spaces.Box(low=-1, high=1,shape=(9,), dtype=np.int32)\n",
        "    self.state = np.zeros([3,3], dtype=np.int32)\n",
        "\n",
        "  def reset(self):\n",
        "    self.state = np.zeros([3,3], dtype=np.int32)\n",
        "    return np.zeros([3,3], dtype=np.int32).flatten()\n",
        "\n",
        "  def step(self, action):\n",
        "\n",
        "    get_action = lambda n: [n//3, n%3]\n",
        "    # function to check win for both agents\n",
        "    def check_win(state):\n",
        "      # check horizontally\n",
        "      for i in range(3):\n",
        "        if state[i][0] == state[i][1] == state[i][2] != 0:\n",
        "          return state[i][0]\n",
        "      # check vertically\n",
        "      for i in range(3):\n",
        "        if state[0][i] == state[1][i] == state[2][i] != 0:\n",
        "          return state[0][i]\n",
        "      \n",
        "      # check left-right diagonal\n",
        "      if state[0][0] == state[1][1] == state[2][2] != 0:\n",
        "        return state[0][0]\n",
        "      \n",
        "      #check right-left diagonal\n",
        "      if state[2][0] == state[1][1] == state[0][2] != 0:\n",
        "        return state[2][0]\n",
        "\n",
        "      return 0\n",
        "\n",
        "    reward = 0\n",
        "    done = False\n",
        "\n",
        "    # check if there are empty squares\n",
        "    n_zeros = np.count_nonzero(self.state == 0)\n",
        "\n",
        "    # if there are none then game is done\n",
        "    if n_zeros == 0:\n",
        "      done = True\n",
        "    else:\n",
        "      #play the move if possible else game done\n",
        "      action_ = get_action(action)\n",
        "      if self.state[int(action_[0])][int(action_[1])] == 0:\n",
        "        self.state[int(action_[0])][int(action_[1])] = 1\n",
        "      else:\n",
        "        reward-=10\n",
        "        done = True\n",
        "    \n",
        "    #check if agent has won\n",
        "    win = check_win(self.state) == 1\n",
        "    if win:\n",
        "      reward = 10\n",
        "      done = True \n",
        "    \n",
        "    if not done:\n",
        "      # get all indices of empty squares for random agent to play\n",
        "      empty_squares = []\n",
        "      for i in range(3):\n",
        "        for j in range(3):\n",
        "          if self.state[i][j] == 0:\n",
        "            empty_squares.append([i,j])\n",
        "      \n",
        "      if len(empty_squares) == 0:\n",
        "        # there are no empty squares\n",
        "        done = True\n",
        "      else:\n",
        "        #play the move of random agent\n",
        "        agent_move = random.choice(empty_squares)\n",
        "        self.state[agent_move[0]][agent_move[1]] = -1\n",
        "\n",
        "        #check if random agent has won\n",
        "        win = check_win(self.state) == -1\n",
        "        if win:\n",
        "          reward = -5\n",
        "          done = True \n",
        "\n",
        "    info = {}\n",
        "\n",
        "    return self.state.flatten(), reward, done, info\n",
        "\n",
        "  def render(self, mode='console'):\n",
        "        if mode!= 'console':\n",
        "          raise NotImplementedError()\n",
        "        print(self.state)\n",
        "\n",
        "  def close(self):\n",
        "        pass"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines.common.env_checker import check_env\n",
        "env = TicTacToeEnv()\n",
        "check_env(env, warn=True)"
      ],
      "metadata": {
        "id": "ieQCfLxU59yE"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = TicTacToeEnv()\n",
        "for step in range(10):\n",
        "  print(\"Step {}\".format(step + 1))\n",
        "  a = int(input())\n",
        "  obs, reward, done, info = env.step(a)\n",
        "  print('obs=', obs, 'reward=', reward, 'done=', done)\n",
        "  env.render()\n",
        "  if done:\n",
        "    print(\"Goal reached!\", \"reward=\", reward)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNCqZVu8gGNN",
        "outputId": "a7d3381a-5e28-47b8-aef6-56c6d3f411a3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1\n",
            "1\n",
            "obs= [-1  1  0  0  0  0  0  0  0] reward= 0 done= False\n",
            "[[-1  1  0]\n",
            " [ 0  0  0]\n",
            " [ 0  0  0]]\n",
            "Step 2\n",
            "4\n",
            "obs= [-1  1  0  0  1  0  0  0 -1] reward= 0 done= False\n",
            "[[-1  1  0]\n",
            " [ 0  1  0]\n",
            " [ 0  0 -1]]\n",
            "Step 3\n",
            "7\n",
            "obs= [-1  1  0  0  1  0  0  1 -1] reward= 10 done= True\n",
            "[[-1  1  0]\n",
            " [ 0  1  0]\n",
            " [ 0  1 -1]]\n",
            "Goal reached! reward= 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines import DQN, ACKTR\n",
        "from stable_baselines.common.cmd_util import make_vec_env\n",
        "\n",
        "env = TicTacToeEnv()\n",
        "env = make_vec_env(lambda: env, n_envs=1)"
      ],
      "metadata": {
        "id": "g1UJT4JwfBXq"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ACKTR('MlpPolicy', env, verbose=1).learn(5000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkbVfdf6fXTf",
        "outputId": "8afe13f3-8ea7-4f15-8ab2-984c9307fb26"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/policies.py:116: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/policies.py:561: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/tf_layers.py:123: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/distributions.py:326: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/distributions.py:327: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/tf_util.py:449: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/common/tf_util.py:449: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/acktr/acktr.py:177: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/acktr/acktr.py:182: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/acktr/kfac.py:99: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/acktr/kfac.py:298: The name tf.diag is deprecated. Please use tf.linalg.tensor_diag instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/acktr/kfac.py:546: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/acktr/kfac.py:548: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/acktr/acktr.py:222: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/acktr/acktr.py:224: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/acktr/acktr.py:307: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/acktr/acktr.py:308: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/acktr/kfac.py:973: The name tf.train.MomentumOptimizer is deprecated. Please use tf.compat.v1.train.MomentumOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/acktr/kfac.py:914: The name tf.mod is deprecated. Please use tf.math.mod instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/acktr/kfac.py:621: The name tf.self_adjoint_eig is deprecated. Please use tf.linalg.eigh instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/stable_baselines/acktr/acktr.py:320: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------\n",
            "| ep_len_mean        | 3.6      |\n",
            "| ep_reward_mean     | -7       |\n",
            "| explained_variance | 0.0364   |\n",
            "| fps                | 12       |\n",
            "| nupdates           | 1        |\n",
            "| policy_entropy     | 2.2      |\n",
            "| policy_loss        | -13.9    |\n",
            "| total_timesteps    | 20       |\n",
            "| value_loss         | 49.2     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 3.07     |\n",
            "| ep_reward_mean     | -6.05    |\n",
            "| explained_variance | -0.27    |\n",
            "| fps                | 373      |\n",
            "| nupdates           | 100      |\n",
            "| policy_entropy     | 2.11     |\n",
            "| policy_loss        | -0.276   |\n",
            "| total_timesteps    | 2000     |\n",
            "| value_loss         | 4.07     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| ep_len_mean        | 2.85     |\n",
            "| ep_reward_mean     | 0.25     |\n",
            "| explained_variance | 0.128    |\n",
            "| fps                | 435      |\n",
            "| nupdates           | 200      |\n",
            "| policy_entropy     | 0.443    |\n",
            "| policy_loss        | 2.74     |\n",
            "| total_timesteps    | 4000     |\n",
            "| value_loss         | 105      |\n",
            "---------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "obs = env.reset()\n",
        "n_steps = 20\n",
        "for step in range(n_steps):\n",
        "  action, _ = model.predict(obs, deterministic=True)\n",
        "  print(\"Step {}\".format(step + 1))\n",
        "  print(\"Action: \", action)\n",
        "  obs, reward, done, info = env.step(action)\n",
        "  print('obs=', obs, 'reward=', reward, 'done=', done)\n",
        "  env.render(mode='console')\n",
        "  if done:\n",
        "    # Note that the VecEnv resets automatically\n",
        "    # when a done signal is encountered\n",
        "    print(\"Goal reached!\", \"reward=\", reward)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rH37Ny5f_LTk",
        "outputId": "821e66b6-6e28-429b-8544-b1a8a5f6242c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1\n",
            "Action:  [4]\n",
            "obs= [[ 0  0  0  0  1  0  0  0 -1]] reward= [0.] done= [False]\n",
            "[[ 0  0  0]\n",
            " [ 0  1  0]\n",
            " [ 0  0 -1]]\n",
            "Step 2\n",
            "Action:  [3]\n",
            "obs= [[ 0  0  0  1  1  0  0 -1 -1]] reward= [0.] done= [False]\n",
            "[[ 0  0  0]\n",
            " [ 1  1  0]\n",
            " [ 0 -1 -1]]\n",
            "Step 3\n",
            "Action:  [5]\n",
            "obs= [[0 0 0 0 0 0 0 0 0]] reward= [10.] done= [ True]\n",
            "[[0 0 0]\n",
            " [0 0 0]\n",
            " [0 0 0]]\n",
            "Goal reached! reward= [10.]\n"
          ]
        }
      ]
    }
  ]
}